{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e8f624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, RobustScaler \n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Dense, LSTM, Conv1D, MaxPooling1D, Bidirectional,\n",
    "    BatchNormalization, Dropout, Concatenate, MultiHeadAttention,\n",
    "    LayerNormalization, GlobalAveragePooling1D, Add, Embedding, Layer\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "STOCK_LIST = [\n",
    "    'ACB', 'BID', 'CTG', 'HDB', 'LPB', 'MBB', 'SHB',\n",
    "    'STB', 'TCB', 'TPB', 'VCB', 'VIB'\n",
    "]\n",
    "BASE_DATA_DIR = \"vnstock_data\" \n",
    "PRIMARY_TIMEFRAME = '5m'\n",
    "REQUIRED_OHLCV_COLS = ['open', 'high', 'low', 'close', 'volume'] \n",
    "PREPROCESS_DIR = 'preprocessed_data' \n",
    "MODEL_DIR = 'models' \n",
    "WINDOW_SIZE = 21 \n",
    "HORIZON = 1     \n",
    "TARGET_COL_NAME = 'close' \n",
    "USE_ROBUST_SCALER = False \n",
    "\n",
    "os.makedirs(PREPROCESS_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Đã nhập thư viện và định nghĩa các hằng số.\")\n",
    "\n",
    "# Hàm Tải và Làm sạch Dữ liệu Ban đầu\n",
    "\n",
    "def load_and_initial_clean(symbol):\n",
    "    file_path = os.path.join(BASE_DATA_DIR, symbol, f\"{symbol}_history_{PRIMARY_TIMEFRAME}.csv\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    df.sort_index(inplace=True)\n",
    "    df = df[REQUIRED_OHLCV_COLS]\n",
    "\n",
    "    for col in REQUIRED_OHLCV_COLS:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    initial_rows = len(df)\n",
    "    df.dropna(subset=REQUIRED_OHLCV_COLS, inplace=True)\n",
    "    rows_after_dropna = len(df)\n",
    "    if initial_rows > rows_after_dropna:\n",
    "         print(f\"[{symbol}] Đã loại bỏ {initial_rows - rows_after_dropna} hàng có NaN trong các cột thiết yếu.\")\n",
    "\n",
    "    print(f\"Hoàn tất làm sạch ban đầu cho {symbol}. Shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "print(\"Đã định nghĩa hàm tải và làm sạch dữ liệu ban đầu.\")\n",
    "\n",
    "\n",
    "# Tải Toàn bộ Dữ liệu & Tạo Đặc trưng Kỹ thuật\n",
    "\n",
    "df_dict_raw = {}\n",
    "successful_symbols_load = []\n",
    "for symbol in STOCK_LIST:\n",
    "    try:\n",
    "        cleaned_df = load_and_initial_clean(symbol)\n",
    "        if cleaned_df is not None and not cleaned_df.empty:\n",
    "            df_dict_raw[symbol] = cleaned_df\n",
    "            successful_symbols_load.append(symbol)\n",
    "        else:\n",
    "             print(f\"--- Không thể tải hoặc làm sạch ban đầu cho mã {symbol} ---\")\n",
    "    except FileNotFoundError:\n",
    "         print(f\"Cảnh báo: Không tìm thấy file cho mã {symbol}. Bỏ qua.\")\n",
    "    except Exception as e:\n",
    "         print(f\"Lỗi khi tải/làm sạch mã {symbol}: {e}\")\n",
    "\n",
    "STOCK_LIST = successful_symbols_load\n",
    "print(f\"\\nĐã tải và làm sạch ban đầu cho {len(STOCK_LIST)} mã cổ phiếu: {STOCK_LIST}\")\n",
    "\n",
    "def add_technical_indicators(df, symbol_name=\"\"):\n",
    "    df_with_indicators = df.copy()\n",
    "    close = df_with_indicators['close']\n",
    "    high = df_with_indicators['high']\n",
    "    low = df_with_indicators['low']\n",
    "    volume = df_with_indicators['volume'].clip(lower=1)\n",
    "\n",
    "# SMA\n",
    "    df_with_indicators['sma_5'] = close.rolling(window=5).mean()\n",
    "    df_with_indicators['sma_20'] = close.rolling(window=20).mean()\n",
    "# EMA\n",
    "    df_with_indicators['ema_12'] = close.ewm(span=12, adjust=False).mean()\n",
    "    df_with_indicators['ema_26'] = close.ewm(span=26, adjust=False).mean()\n",
    "# Bollinger Bands\n",
    "    bb_mid = close.rolling(window=20).mean()\n",
    "    bb_std = close.rolling(window=20).std()\n",
    "    df_with_indicators['bb_mid'] = bb_mid\n",
    "    df_with_indicators['bb_high'] = bb_mid + 2 * bb_std\n",
    "    df_with_indicators['bb_low'] = bb_mid - 2 * bb_std\n",
    "    bb_mid_safe = bb_mid.replace(0, np.nan) # Avoid division by zero\n",
    "    df_with_indicators['bb_width'] = (df_with_indicators['bb_high'] - df_with_indicators['bb_low']) / bb_mid_safe\n",
    "# RSI\n",
    "    delta = close.diff()\n",
    "    gain = delta.where(delta > 0, 0).rolling(window=14).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "    loss_safe = loss.replace(0, 1e-6) # Avoid division by zero\n",
    "    rs = gain / loss_safe\n",
    "    df_with_indicators['rsi'] = 100 - (100 / (1 + rs))\n",
    "    df_with_indicators['rsi'] = df_with_indicators['rsi'].fillna(50) # Fill initial NaNs\n",
    "    # MACD\n",
    "    exp1 = df_with_indicators['ema_12']\n",
    "    exp2 = df_with_indicators['ema_26']\n",
    "    df_with_indicators['macd'] = exp1 - exp2\n",
    "    df_with_indicators['macd_signal'] = df_with_indicators['macd'].ewm(span=9, adjust=False).mean()\n",
    "    df_with_indicators['macd_diff'] = df_with_indicators['macd'] - df_with_indicators['macd_signal']\n",
    "# Stochastic Oscillator\n",
    "    low_min = low.rolling(window=14).min()\n",
    "    high_max = high.rolling(window=14).max()\n",
    "    stoch_range = (high_max - low_min).replace(0, np.nan) # Avoid division by zero\n",
    "    df_with_indicators['%K'] = (close - low_min) * 100 / stoch_range\n",
    "    df_with_indicators['%D'] = df_with_indicators['%K'].rolling(window=3).mean()\n",
    "    df_with_indicators['%K'] = df_with_indicators['%K'].fillna(50) \n",
    "    df_with_indicators['%D'] = df_with_indicators['%D'].fillna(50) \n",
    "    # ATR\n",
    "    high_low = high - low\n",
    "    high_close_prev = abs(high - close.shift())\n",
    "    low_close_prev = abs(low - close.shift())\n",
    "    ranges = pd.concat([high_low, high_close_prev, low_close_prev], axis=1)\n",
    "    true_range = ranges.max(axis=1)\n",
    "    df_with_indicators['atr'] = true_range.rolling(window=14).mean()\n",
    "# ROC\n",
    "    df_with_indicators['roc'] = close.pct_change(periods=12) * 100\n",
    "# OBV\n",
    "    df_with_indicators['obv'] = (np.sign(close.diff()).fillna(0) * volume).cumsum()\n",
    "# Volume SMA\n",
    "    df_with_indicators['volume_sma'] = volume.rolling(window=20).mean()\n",
    "# Volatility (Standard Deviation of returns)\n",
    "    df_with_indicators['volatility'] = close.pct_change().rolling(window=10).std() * 100\n",
    "\n",
    "\n",
    "    # Loại bỏ các hàng có NaN được tạo ra bởi các cửa sổ trượt\n",
    "    initial_len = len(df_with_indicators)\n",
    "    df_with_indicators.dropna(inplace=True)\n",
    "    final_len = len(df_with_indicators)\n",
    "    if initial_len > final_len:\n",
    "        print(f\"[{symbol_name}] Đã loại bỏ {initial_len - final_len} hàng đầu tiên do NaN từ chỉ báo.\")\n",
    "    return df_with_indicators\n",
    "\n",
    "data_with_indicators = {}\n",
    "successful_symbols_indicators = []\n",
    "for symbol in STOCK_LIST:\n",
    "    print(f'\\nĐang thêm các chỉ báo kỹ thuật cho mã {symbol}...')\n",
    "    df_raw = df_dict_raw[symbol] \n",
    "    processed_df = add_technical_indicators(df_raw, symbol_name=symbol)\n",
    "    data_with_indicators[symbol] = processed_df\n",
    "    successful_symbols_indicators.append(symbol)\n",
    "    print(f\"Đã thêm chỉ báo cho {symbol}. Shape: {processed_df.shape}\")\n",
    "\n",
    "STOCK_LIST = successful_symbols_indicators\n",
    "print(f'\\nĐã thêm các chỉ báo kỹ thuật. Số mã cổ phiếu có thể sử dụng: {len(STOCK_LIST)}')\n",
    "\n",
    "# Chia Dữ liệu (Train/Validation/Test)\n",
    "train_data = {}\n",
    "val_data = {}\n",
    "test_data = {}\n",
    "successful_symbols_split = []\n",
    "\n",
    "for symbol in STOCK_LIST:\n",
    "    df = data_with_indicators[symbol] \n",
    "    n = len(df)\n",
    "\n",
    "# Chia theo tỷ lệ 80% train, 10% val, 10% test\n",
    "    train_size = int(0.8 * n)\n",
    "    val_size = int(0.1 * n)\n",
    "\n",
    "    train_data[symbol] = df.iloc[:train_size].copy()\n",
    "    val_data[symbol] = df.iloc[train_size : train_size + val_size].copy()\n",
    "    test_data[symbol] = df.iloc[train_size + val_size :].copy()\n",
    "    print(f\"Đã chia mã {symbol}: Train={len(train_data[symbol])}, Val={len(val_data[symbol])}, Test={len(test_data[symbol])}\")\n",
    "    successful_symbols_split.append(symbol)\n",
    "\n",
    "STOCK_LIST = successful_symbols_split\n",
    "print(f'\\nHoàn tất chia dữ liệu cho {len(STOCK_LIST)} mã cổ phiếu: {STOCK_LIST}')\n",
    "\n",
    "# Xử lý Missing Value SAU KHI Chia Tách \n",
    "\n",
    "def apply_ffill(data_dict):\n",
    "    filled_dict = {}\n",
    "    print(\"Áp dụng forward fill (ffill) cho các tập dữ liệu...\")\n",
    "    for symbol, df in data_dict.items():\n",
    "        df_filled = df.copy()\n",
    "        df_filled.ffill(inplace=True)\n",
    "        if df_filled.isna().sum().sum() > 0:\n",
    "            print(f\"[{symbol}] Vẫn còn NaN sau ffill, áp dụng bfill...\")\n",
    "            df_filled.bfill(inplace=True)\n",
    "        filled_dict[symbol] = df_filled\n",
    "    print(\"Hoàn tất xử lý NaN sau khi chia tách.\")\n",
    "    return filled_dict\n",
    "\n",
    "train_data = apply_ffill(train_data)\n",
    "val_data = apply_ffill(val_data)\n",
    "test_data = apply_ffill(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8e0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chuẩn hóa Dữ liệu (Scaling) và Mã hóa One-Hot (Encoding)\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        if USE_ROBUST_SCALER:\n",
    "            self.scaler = RobustScaler()\n",
    "            print(\"Sử dụng RobustScaler.\")\n",
    "        else:\n",
    "            self.scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "            print(\"Sử dụng MinMaxScaler.\")\n",
    "        self.encoder = OneHotEncoder(categories=[STOCK_LIST], sparse_output=False, handle_unknown='ignore', dtype=np.float32)\n",
    "        self.numeric_columns_ = None\n",
    "        self.encoded_feature_names_ = None\n",
    "        self.n_features_in_ = None\n",
    "        self.fitted_ = False\n",
    "        self.target_col_index_in_numeric_ = -1\n",
    "\n",
    "    def _prepare_combined_data(self, data_dict):\n",
    "        all_data = []\n",
    "        original_indices = {}\n",
    "        current_pos = 0\n",
    "        valid_symbols_in_dict = [s for s in STOCK_LIST if s in data_dict and not data_dict[s].empty]\n",
    "\n",
    "        for symbol in valid_symbols_in_dict:\n",
    "            df = data_dict[symbol].copy()\n",
    "            df['symbol_cat_temp'] = symbol\n",
    "            all_data.append(df)\n",
    "            original_indices[symbol] = (current_pos, current_pos + len(df))\n",
    "            current_pos += len(df)\n",
    "\n",
    "        combined_data = pd.concat(all_data, axis=0, ignore_index=False)\n",
    "        return combined_data, original_indices, valid_symbols_in_dict\n",
    "\n",
    "    def fit_transform(self, data_dict):\n",
    "        combined_data, original_indices, valid_symbols = self._prepare_combined_data(data_dict)\n",
    "\n",
    "        self.numeric_columns_ = combined_data.select_dtypes(include=np.number).columns.tolist()\n",
    "        if 'symbol_cat_temp' in self.numeric_columns_:\n",
    "            self.numeric_columns_.remove('symbol_cat_temp')\n",
    "        print(f\"Đã xác định {len(self.numeric_columns_)} cột số để chuẩn hóa.\")\n",
    "\n",
    "        self.target_col_index_in_numeric_ = self.numeric_columns_.index(TARGET_COL_NAME)\n",
    "        print(f\"Cột target '{TARGET_COL_NAME}' có index {self.target_col_index_in_numeric_}.\")\n",
    "\n",
    "        numeric_data = combined_data[self.numeric_columns_].values\n",
    "        scaled_numeric_values = self.scaler.fit_transform(numeric_data)\n",
    "        self.n_features_in_ = self.scaler.n_features_in_\n",
    "        scaled_numeric_df = pd.DataFrame(scaled_numeric_values, columns=self.numeric_columns_, index=combined_data.index)\n",
    "\n",
    "        symbols_array = combined_data[['symbol_cat_temp']]\n",
    "        self.encoder.fit(pd.DataFrame({'symbol_cat_temp': STOCK_LIST}))\n",
    "        symbol_encoded_values = self.encoder.transform(symbols_array)\n",
    "        self.encoded_feature_names_ = self.encoder.get_feature_names_out(['symbol'])\n",
    "        encoded_df = pd.DataFrame(symbol_encoded_values, columns=self.encoded_feature_names_, index=combined_data.index)\n",
    "\n",
    "        final_df = pd.concat([scaled_numeric_df, encoded_df], axis=1)\n",
    "        print(f\"Shape của đặc trưng kết hợp sau khi fit_transform: {final_df.shape}\")\n",
    "\n",
    "        transformed_dict = {}\n",
    "        for symbol, (start_idx, end_idx) in original_indices.items():\n",
    "            symbol_indices = combined_data.iloc[start_idx:end_idx].index\n",
    "            transformed_dict[symbol] = final_df.loc[symbol_indices].copy()\n",
    "\n",
    "        print(\"Đang lưu trạng thái scaler, encoder và danh sách cột...\")\n",
    "        joblib.dump(self.scaler, os.path.join(PREPROCESS_DIR, 'feature_scaler.pkl'))\n",
    "        joblib.dump(self.encoder, os.path.join(PREPROCESS_DIR, 'symbol_encoder.pkl'))\n",
    "        joblib.dump(self.numeric_columns_, os.path.join(PREPROCESS_DIR, 'numeric_columns.pkl'))\n",
    "        joblib.dump(self.encoded_feature_names_, os.path.join(PREPROCESS_DIR, 'encoded_feature_names.pkl'))\n",
    "        joblib.dump(self.target_col_index_in_numeric_, os.path.join(PREPROCESS_DIR, 'target_col_index.pkl'))\n",
    "        self.fitted_ = True\n",
    "        print(\"Đã fit và lưu Scaler, Encoder, và thông tin cột.\")\n",
    "        return transformed_dict\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        if not self.fitted_:\n",
    "            print(\"Bộ xử lý chưa được fit. Đang tải trạng thái đã lưu...\")\n",
    "            self.scaler = joblib.load(os.path.join(PREPROCESS_DIR, 'feature_scaler.pkl'))\n",
    "            self.encoder = joblib.load(os.path.join(PREPROCESS_DIR, 'symbol_encoder.pkl'))\n",
    "            self.numeric_columns_ = joblib.load(os.path.join(PREPROCESS_DIR, 'numeric_columns.pkl'))\n",
    "            self.encoded_feature_names_ = joblib.load(os.path.join(PREPROCESS_DIR, 'encoded_feature_names.pkl'))\n",
    "            self.target_col_index_in_numeric_ = joblib.load(os.path.join(PREPROCESS_DIR, 'target_col_index.pkl'))\n",
    "            self.n_features_in_ = self.scaler.n_features_in_\n",
    "            self.fitted_ = True\n",
    "            print(\"Đã tải scaler và encoder đã được fit trước đó.\")\n",
    "\n",
    "        combined_data, original_indices, valid_symbols = self._prepare_combined_data(data_dict)\n",
    "\n",
    "        numeric_data = combined_data[self.numeric_columns_].values\n",
    "        scaled_numeric_values = self.scaler.transform(numeric_data)\n",
    "        scaled_numeric_df = pd.DataFrame(scaled_numeric_values, columns=self.numeric_columns_, index=combined_data.index)\n",
    "\n",
    "        symbols_array = combined_data[['symbol_cat_temp']]\n",
    "        symbol_encoded_values = self.encoder.transform(symbols_array)\n",
    "        encoded_df = pd.DataFrame(symbol_encoded_values, columns=self.encoded_feature_names_, index=combined_data.index)\n",
    "\n",
    "        final_df = pd.concat([scaled_numeric_df, encoded_df], axis=1)\n",
    "        print(f\"Shape của đặc trưng kết hợp sau khi transform: {final_df.shape}\")\n",
    "\n",
    "        transformed_dict = {}\n",
    "        for symbol, (start_idx, end_idx) in original_indices.items():\n",
    "            symbol_indices = combined_data.iloc[start_idx:end_idx].index\n",
    "            transformed_dict[symbol] = final_df.loc[symbol_indices].copy()\n",
    "\n",
    "        return transformed_dict\n",
    "\n",
    "    def inverse_transform_target(self, scaled_target_values):\n",
    "        num_scaler_features = self.n_features_in_\n",
    "        scaled_values_flat = np.array(scaled_target_values).flatten()\n",
    "        dummy_array = np.zeros((len(scaled_values_flat), num_scaler_features))\n",
    "        dummy_array[:, self.target_col_index_in_numeric_] = scaled_values_flat\n",
    "        inversed_array = self.scaler.inverse_transform(dummy_array)\n",
    "        return inversed_array[:, self.target_col_index_in_numeric_]\n",
    "\n",
    "data_processor = DataProcessor()\n",
    "scaled_encoded_train_data = None\n",
    "scaled_encoded_val_data = None\n",
    "scaled_encoded_test_data = None\n",
    "sequence_feature_names = []\n",
    "\n",
    "print(\"\\nĐang fit và transform dữ liệu training...\")\n",
    "scaled_encoded_train_data = data_processor.fit_transform(train_data)\n",
    "example_symbol = next(iter(scaled_encoded_train_data))\n",
    "sequence_feature_names = scaled_encoded_train_data[example_symbol].columns.tolist()\n",
    "\n",
    "print(\"\\nĐang transform dữ liệu validation...\")\n",
    "scaled_encoded_val_data = data_processor.transform(val_data)\n",
    "print(\"\\nĐang transform dữ liệu test...\")\n",
    "scaled_encoded_test_data = data_processor.transform(test_data)\n",
    "\n",
    "print('\\nHoàn tất chuẩn hóa (scaling) và mã hóa (encoding) dữ liệu.')\n",
    "example_symbol = next(iter(scaled_encoded_train_data))\n",
    "print(f\"\\nVí dụ shape đã xử lý cho mã {example_symbol} (Train): {scaled_encoded_train_data[example_symbol].shape}\")\n",
    "print(f\"Tổng số features cho mô hình sequence: {len(sequence_feature_names)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a64b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_true, y_pred, title, plot_limit=200):\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    limit = min(plot_limit, len(y_true), len(y_pred))\n",
    "    plt.plot(y_true[:limit], label='Giá thực tế', color='blue', marker='.', linestyle='-')\n",
    "    plt.plot(y_pred[:limit], label='Giá dự đoán', color='red', marker='x', linestyle='--')\n",
    "    plt.title(f'{title} ({limit} điểm đầu tiên)')\n",
    "    plt.ylabel('Giá (Thang đo gốc)')\n",
    "    plt.xlabel('Bước thời gian (Tập Test)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name, symbol=\"Overall\"):\n",
    "    if len(y_true) != len(y_pred):\n",
    "        print(f\"[{model_name} - {symbol}] Cảnh báo: Độ dài y_true ({len(y_true)}) và y_pred ({len(y_pred)}) không khớp. Cắt về độ dài ngắn hơn.\")\n",
    "        min_len = min(len(y_true), len(y_pred))\n",
    "        y_true = y_true[:min_len]\n",
    "        y_pred = y_pred[:min_len]\n",
    "\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    print(f\"Kết quả Đánh giá {model_name} cho [{symbol}] (Thang đo gốc):\")\n",
    "    print(f\"  R2: {r2:.4f}, MSE: {mse:.4f}, MAE: {mae:.4f}\")\n",
    "    return {'r2': r2, 'mse': mse, 'mae': mae}\n",
    "\n",
    "class StockDataSequenceFlattened(Sequence):\n",
    "    def __init__(self, data_dict, window_size, horizon, batch_size,\n",
    "                 target_col_name='close', feature_list=None, shuffle=True):\n",
    "        self.data_dict = data_dict\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.target_col_name = target_col_name\n",
    "        self.shuffle = shuffle\n",
    "        self.feature_list = feature_list\n",
    "\n",
    "        self.stock_data_arrays = {symbol: df.values for symbol, df in self.data_dict.items() if not df.empty}\n",
    "        self.valid_symbols = list(self.stock_data_arrays.keys())\n",
    "\n",
    "        if not self.feature_list:\n",
    "            first_symbol = self.valid_symbols[0]\n",
    "            self.feature_list = self.data_dict[first_symbol].columns.tolist()\n",
    "\n",
    "        self.target_col_index_in_features = self.feature_list.index(self.target_col_name)\n",
    "\n",
    "        self.indices = []\n",
    "        for symbol in self.valid_symbols:\n",
    "            n_samples = len(self.stock_data_arrays[symbol])\n",
    "            last_valid_start_index = n_samples - self.window_size - self.horizon\n",
    "            if last_valid_start_index >= 0:\n",
    "                for i in range(last_valid_start_index + 1):\n",
    "                    self.indices.append((symbol, i))\n",
    "\n",
    "        self.n_flat_features = self.window_size * len(self.feature_list)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.indices) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_start = index * self.batch_size\n",
    "        batch_end = (index + 1) * self.batch_size\n",
    "        batch_index_tuples = self.indices[batch_start:batch_end]\n",
    "\n",
    "        X_batch_flat = np.zeros((len(batch_index_tuples), self.n_flat_features), dtype=np.float32)\n",
    "        y_batch = np.zeros(len(batch_index_tuples), dtype=np.float32)\n",
    "\n",
    "        for i, (symbol, start_idx) in enumerate(batch_index_tuples):\n",
    "            stock_values = self.stock_data_arrays[symbol]\n",
    "            x_seq_3d = stock_values[start_idx : start_idx + self.window_size, :]\n",
    "            X_batch_flat[i, :] = x_seq_3d.reshape(-1)\n",
    "            target_time_index = start_idx + self.window_size + self.horizon - 1\n",
    "            y_batch[i] = stock_values[target_time_index, self.target_col_index_in_features]\n",
    "\n",
    "        return X_batch_flat, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def get_all_targets(self):\n",
    "        all_y = np.zeros(len(self.indices), dtype=np.float32)\n",
    "        for i, (symbol, start_idx) in enumerate(self.indices):\n",
    "            stock_values = self.stock_data_arrays[symbol]\n",
    "            target_time_index = start_idx + self.window_size + self.horizon - 1\n",
    "            all_y[i] = stock_values[target_time_index, self.target_col_index_in_features]\n",
    "        return all_y\n",
    "\n",
    "class StockDataSequence(Sequence):\n",
    "    def __init__(self, data_dict, window_size, horizon, batch_size,\n",
    "                 target_col_name='close', feature_list=None, shuffle=True):\n",
    "        self.data_dict = data_dict\n",
    "        self.window_size = window_size\n",
    "        self.horizon = horizon\n",
    "        self.batch_size = batch_size\n",
    "        self.target_col_name = target_col_name\n",
    "        self.shuffle = shuffle\n",
    "        self.feature_list = feature_list\n",
    "\n",
    "        self.stock_data_arrays = {symbol: df.values for symbol, df in self.data_dict.items() if not df.empty}\n",
    "        self.valid_symbols = list(self.stock_data_arrays.keys())\n",
    "\n",
    "        if not self.feature_list:\n",
    "            first_symbol = self.valid_symbols[0]\n",
    "            self.feature_list = self.data_dict[first_symbol].columns.tolist()\n",
    "\n",
    "        self.target_col_index_in_features = self.feature_list.index(self.target_col_name)\n",
    "\n",
    "        self.indices = []\n",
    "        for symbol in self.valid_symbols:\n",
    "            n_samples = len(self.stock_data_arrays[symbol])\n",
    "            last_valid_start_index = n_samples - self.window_size - self.horizon\n",
    "            if last_valid_start_index >= 0:\n",
    "                for i in range(last_valid_start_index + 1):\n",
    "                    self.indices.append((symbol, i))\n",
    "\n",
    "        self.num_features = len(self.feature_list)\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.indices) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_start = index * self.batch_size\n",
    "        batch_end = (index + 1) * self.batch_size\n",
    "        batch_index_tuples = self.indices[batch_start:batch_end]\n",
    "\n",
    "        X_batch = np.zeros((len(batch_index_tuples), self.window_size, self.num_features), dtype=np.float32)\n",
    "        y_batch = np.zeros(len(batch_index_tuples), dtype=np.float32)\n",
    "\n",
    "        for i, (symbol, start_idx) in enumerate(batch_index_tuples):\n",
    "            stock_values = self.stock_data_arrays[symbol]\n",
    "            X_batch[i, :, :] = stock_values[start_idx : start_idx + self.window_size, :]\n",
    "            target_time_index = start_idx + self.window_size + self.horizon - 1\n",
    "            y_batch[i] = stock_values[target_time_index, self.target_col_index_in_features]\n",
    "\n",
    "        return X_batch, y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(self.indices)\n",
    "\n",
    "    def get_all_targets(self):\n",
    "        all_y = np.zeros(len(self.indices), dtype=np.float32)\n",
    "        for i, (symbol, start_idx) in enumerate(self.indices):\n",
    "            stock_values = self.stock_data_arrays[symbol]\n",
    "            target_time_index = start_idx + self.window_size + self.horizon - 1\n",
    "            all_y[i] = stock_values[target_time_index, self.target_col_index_in_features]\n",
    "        return all_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943c2b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_model = None\n",
    "sgd_model_path = os.path.join(MODEL_DIR, 'sgd_regressor_model.pkl')\n",
    "sgd_results_per_stock = {}\n",
    "\n",
    "sgd_train_batch_size = 1024\n",
    "sgd_train_generator = StockDataSequenceFlattened(\n",
    "    data_dict=scaled_encoded_train_data, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "    batch_size=sgd_train_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "    feature_list=sequence_feature_names, shuffle=True\n",
    ")\n",
    "\n",
    "print(f\"SGD Train generator có {len(sgd_train_generator)} batches.\")\n",
    "sgd_model = SGDRegressor(loss='squared_error', penalty='l2', alpha=0.0001, max_iter=1,\n",
    "                         shuffle=False, random_state=42, learning_rate='adaptive', eta0=0.01, tol=None)\n",
    "\n",
    "n_epochs_sgd = 5\n",
    "print(f\"Đang huấn luyện SGDRegressor trong {n_epochs_sgd} epochs...\")\n",
    "for epoch in range(n_epochs_sgd):\n",
    "    print(f\"SGD Epoch {epoch + 1}/{n_epochs_sgd}\")\n",
    "    for i in range(len(sgd_train_generator)):\n",
    "        X_batch_flat, y_batch = sgd_train_generator[i]\n",
    "        sgd_model.partial_fit(X_batch_flat, y_batch)\n",
    "    sgd_train_generator.on_epoch_end()\n",
    "\n",
    "print(\"Hoàn tất huấn luyện SGDRegressor.\")\n",
    "joblib.dump(sgd_model, sgd_model_path)\n",
    "print(f\"Đã lưu mô hình SGDRegressor vào: {sgd_model_path}\")\n",
    "\n",
    "print(f\"\\nĐang tải mô hình SGDRegressor từ: {sgd_model_path}...\")\n",
    "sgd_model_loaded = joblib.load(sgd_model_path)\n",
    "print(\"Bắt đầu đánh giá SGDRegressor...\")\n",
    "\n",
    "all_y_true_sgd = []\n",
    "all_y_pred_sgd = []\n",
    "sgd_test_batch_size = 2048\n",
    "\n",
    "print(\"\\n--- Đánh giá SGD trên từng công ty ---\")\n",
    "available_test_symbols = [s for s in STOCK_LIST if s in scaled_encoded_test_data and not scaled_encoded_test_data[s].empty]\n",
    "print(f\"Các mã có dữ liệu test để đánh giá SGD: {available_test_symbols}\")\n",
    "\n",
    "for symbol in available_test_symbols:\n",
    "    print(f\"\\nĐang đánh giá cho mã: {symbol}\")\n",
    "    print(f\"Shape của scaled_encoded_test_data['{symbol}']: {scaled_encoded_test_data[symbol].shape}\")\n",
    "    temp_test_dict = {symbol: scaled_encoded_test_data[symbol]}\n",
    "\n",
    "    sgd_test_generator_single = StockDataSequenceFlattened(\n",
    "        data_dict=temp_test_dict, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "        batch_size=sgd_test_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "        feature_list=sequence_feature_names, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_test_true_scaled_single = sgd_test_generator_single.get_all_targets()\n",
    "    print(f\"[{symbol}] Số lượng điểm test (targets): {len(y_test_true_scaled_single)}\")\n",
    "\n",
    "    y_pred_sgd_scaled_single_list = []\n",
    "    print(f\"[{symbol}] Đang dự đoán...\")\n",
    "    for i in range(len(sgd_test_generator_single)):\n",
    "         X_batch_flat, _ = sgd_test_generator_single[i]\n",
    "         y_pred_batch = sgd_model_loaded.predict(X_batch_flat)\n",
    "         y_pred_sgd_scaled_single_list.append(y_pred_batch)\n",
    "\n",
    "    y_pred_sgd_scaled_single = np.concatenate(y_pred_sgd_scaled_single_list)\n",
    "    print(f\"[{symbol}] Số lượng dự đoán: {len(y_pred_sgd_scaled_single)}\")\n",
    "\n",
    "    print(f\"[{symbol}] Đang inverse transform...\")\n",
    "    y_true_inv_single = data_processor.inverse_transform_target(y_test_true_scaled_single)\n",
    "    y_pred_inv_single = data_processor.inverse_transform_target(y_pred_sgd_scaled_single)\n",
    "    print(f\"[{symbol}] Inverse transform hoàn tất.\")\n",
    "\n",
    "    metrics = evaluate_model(y_true_inv_single, y_pred_inv_single, \"SGDRegressor\", symbol)\n",
    "    sgd_results_per_stock[symbol] = {'metrics': metrics, 'y_true': y_true_inv_single, 'y_pred': y_pred_inv_single}\n",
    "\n",
    "    print(f\"[{symbol}] Đang vẽ biểu đồ dự đoán...\")\n",
    "    plot_predictions(y_true_inv_single, y_pred_inv_single, f'SGDRegressor: {symbol} - Giá Thực tế vs. Dự đoán')\n",
    "\n",
    "    all_y_true_sgd.extend(y_true_inv_single)\n",
    "    all_y_pred_sgd.extend(y_pred_inv_single)\n",
    "\n",
    "print(\"\\n--- Đánh giá SGD Tổng thể (Overall) ---\")\n",
    "print(f\"Tổng số điểm đánh giá overall: {len(all_y_true_sgd)}\")\n",
    "overall_metrics_sgd = evaluate_model(np.array(all_y_true_sgd), np.array(all_y_pred_sgd), \"SGDRegressor\", \"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabe0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_hybrid_cnn_lstm_attention_model(input_shape, lstm_units=64, cnn_filters_1=64, cnn_filters_2=128, kernel_size=3, num_heads=8, key_dim=64, dropout_rate=0.3, l2_reg=0.001):\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    cnn = Conv1D(filters=cnn_filters_1, kernel_size=kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    cnn = BatchNormalization()(cnn)\n",
    "    cnn = Conv1D(filters=cnn_filters_2, kernel_size=kernel_size, activation='relu', padding='same', kernel_regularizer=l2(l2_reg))(cnn)\n",
    "    cnn_output = BatchNormalization()(cnn)\n",
    "\n",
    "    lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, kernel_regularizer=l2(l2_reg)))(inputs)\n",
    "    lstm_output = BatchNormalization()(lstm)\n",
    "\n",
    "    concatenated_features = Concatenate(axis=-1)([lstm_output, cnn_output])\n",
    "    norm_features = LayerNormalization(epsilon=1e-6)(concatenated_features)\n",
    "    attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim, dropout=dropout_rate)(query=norm_features, value=norm_features, key=norm_features)\n",
    "    attention_output = Dropout(dropout_rate)(attention_output)\n",
    "    attention_output = LayerNormalization(epsilon=1e-6)(attention_output)\n",
    "\n",
    "    pooled_output = GlobalAveragePooling1D()(attention_output)\n",
    "    x = BatchNormalization()(pooled_output)\n",
    "    x = Dense(128, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='linear', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "print(\"Đang xác định input shape và xây dựng mô hình Deep Learning (CNN-BiLSTM-Attention)...\")\n",
    "num_features_dl = len(sequence_feature_names)\n",
    "dl_input_shape = (WINDOW_SIZE, num_features_dl)\n",
    "print(f\"Shape đầu vào cho mô hình DL: {dl_input_shape}\")\n",
    "dl_model = build_hybrid_cnn_lstm_attention_model(dl_input_shape)\n",
    "\n",
    "dl_batch_size = 64\n",
    "train_generator_dl = StockDataSequence(\n",
    "    data_dict=scaled_encoded_train_data, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "    batch_size=dl_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "    feature_list=sequence_feature_names, shuffle=True\n",
    ")\n",
    "val_generator_dl = StockDataSequence(\n",
    "    data_dict=scaled_encoded_val_data, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "    batch_size=dl_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "    feature_list=sequence_feature_names, shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"DL Train generator có {len(train_generator_dl)} batches.\")\n",
    "print(f\"DL Validation generator có {len(val_generator_dl)} batches.\")\n",
    "\n",
    "history_dl = None\n",
    "dl_model_path = os.path.join(MODEL_DIR, 'best_cnn_bilstm_attention_model.keras')\n",
    "\n",
    "model_checkpoint_dl = ModelCheckpoint(filepath=dl_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "early_stopping_dl = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True, verbose=1)\n",
    "reduce_lr_dl = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, mode='min', min_lr=1e-6, verbose=1)\n",
    "\n",
    "epochs = 100\n",
    "history_dl = dl_model.fit(\n",
    "    train_generator_dl, epochs=epochs,\n",
    "    validation_data=val_generator_dl,\n",
    "    callbacks=[model_checkpoint_dl, early_stopping_dl, reduce_lr_dl],\n",
    "    verbose=1,\n",
    "    workers=4,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_dl.history['loss'], label='Train Loss')\n",
    "plt.plot(history_dl.history['val_loss'], label='Validation Loss')\n",
    "plt.title('CNN-BiLSTM-Attention: Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "dl_results_per_stock = {}\n",
    "\n",
    "print(f\"\\nĐang tải mô hình CNN-BiLSTM-Attention tốt nhất từ: {dl_model_path}...\")\n",
    "dl_model_loaded = load_model(dl_model_path)\n",
    "print(\"Đã tải mô hình CNN-BiLSTM-Attention.\")\n",
    "print(\"Bắt đầu đánh giá CNN-BiLSTM-Attention...\")\n",
    "\n",
    "all_y_true_dl = []\n",
    "all_y_pred_dl = []\n",
    "dl_test_batch_size = 128\n",
    "\n",
    "print(\"\\n--- Đánh giá CNN-BiLSTM-Attention trên từng công ty ---\")\n",
    "for symbol in available_test_symbols:\n",
    "    print(f\"\\nĐang đánh giá cho mã: {symbol}\")\n",
    "    temp_test_dict = {symbol: scaled_encoded_test_data[symbol]}\n",
    "\n",
    "    dl_test_generator_single = StockDataSequence(\n",
    "        data_dict=temp_test_dict, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "        batch_size=dl_test_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "        feature_list=sequence_feature_names, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_test_true_scaled_single = dl_test_generator_single.get_all_targets()\n",
    "    print(f\"[{symbol}] Đang dự đoán...\")\n",
    "    y_pred_dl_scaled_single = dl_model_loaded.predict(dl_test_generator_single)\n",
    "\n",
    "    y_true_inv_single = data_processor.inverse_transform_target(y_test_true_scaled_single)\n",
    "    y_pred_inv_single = data_processor.inverse_transform_target(y_pred_dl_scaled_single)\n",
    "\n",
    "    metrics = evaluate_model(y_true_inv_single, y_pred_inv_single, \"CNN-BiLSTM-Att\", symbol)\n",
    "    dl_results_per_stock[symbol] = {'metrics': metrics, 'y_true': y_true_inv_single, 'y_pred': y_pred_inv_single}\n",
    "\n",
    "    plot_predictions(y_true_inv_single, y_pred_inv_single, f'CNN-BiLSTM-Att: {symbol} - Giá Thực tế vs. Dự đoán')\n",
    "\n",
    "    all_y_true_dl.extend(y_true_inv_single)\n",
    "    all_y_pred_dl.extend(y_pred_inv_single)\n",
    "\n",
    "print(\"\\n--- Đánh giá CNN-BiLSTM-Attention Tổng thể (Overall) ---\")\n",
    "overall_metrics_dl = evaluate_model(np.array(all_y_true_dl), np.array(all_y_pred_dl), \"CNN-BiLSTM-Att\", \"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d5a30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape, lstm_units_1=100, lstm_units_2=50, dropout_rate=0.2, l2_reg=0.001):\n",
    "    \"\"\"Xây dựng mô hình LSTM đơn giản.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = LSTM(lstm_units_1, return_sequences=True, kernel_regularizer=l2(l2_reg))(inputs)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = LSTM(lstm_units_2, return_sequences=False, kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(64, activation='relu', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation='linear', dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "print(\"Đang xây dựng mô hình LSTM...\")\n",
    "lstm_model = build_lstm_model(dl_input_shape)\n",
    "\n",
    "history_lstm = None\n",
    "lstm_model_path = os.path.join(MODEL_DIR, 'best_lstm_model.keras')\n",
    "\n",
    "print(\"\\nSử dụng lại Data Generators (Sequence) cho mô hình LSTM...\")\n",
    "print(f\"LSTM Train generator có {len(train_generator_dl)} batches.\")\n",
    "print(f\"LSTM Validation generator có {len(val_generator_dl)} batches.\")\n",
    "\n",
    "model_checkpoint_lstm = ModelCheckpoint(filepath=lstm_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True, verbose=1)\n",
    "reduce_lr_lstm = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, mode='min', min_lr=1e-6, verbose=1)\n",
    "\n",
    "epochs = 100\n",
    "history_lstm = lstm_model.fit(\n",
    "    train_generator_dl, epochs=epochs,\n",
    "    validation_data=val_generator_dl,\n",
    "    callbacks=[model_checkpoint_lstm, early_stopping_lstm, reduce_lr_lstm],\n",
    "    verbose=1,\n",
    "    workers=4,\n",
    "    use_multiprocessing=True\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history_lstm.history['loss'], label='Train Loss')\n",
    "plt.plot(history_lstm.history['val_loss'], label='Validation Loss')\n",
    "plt.title('LSTM: Training & Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "lstm_results_per_stock = {}\n",
    "\n",
    "print(f\"\\nĐang tải mô hình LSTM tốt nhất từ: {lstm_model_path}...\")\n",
    "lstm_model_loaded = load_model(lstm_model_path)\n",
    "print(\"Đã tải mô hình LSTM.\")\n",
    "print(\"Bắt đầu đánh giá LSTM...\")\n",
    "\n",
    "all_y_true_lstm = []\n",
    "all_y_pred_lstm = []\n",
    "lstm_test_batch_size = 128\n",
    "\n",
    "print(\"\\n--- Đánh giá LSTM trên từng công ty ---\")\n",
    "for symbol in available_test_symbols:\n",
    "    print(f\"\\nĐang đánh giá cho mã: {symbol}\")\n",
    "    temp_test_dict = {symbol: scaled_encoded_test_data[symbol]}\n",
    "\n",
    "    lstm_test_generator_single = StockDataSequence(\n",
    "        data_dict=temp_test_dict, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "        batch_size=lstm_test_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "        feature_list=sequence_feature_names, shuffle=False\n",
    "    )\n",
    "\n",
    "    y_test_true_scaled_single = lstm_test_generator_single.get_all_targets()\n",
    "    print(f\"[{symbol}] Đang dự đoán...\")\n",
    "    y_pred_lstm_scaled_single = lstm_model_loaded.predict(lstm_test_generator_single)\n",
    "\n",
    "    y_true_inv_single = data_processor.inverse_transform_target(y_test_true_scaled_single)\n",
    "    y_pred_inv_single = data_processor.inverse_transform_target(y_pred_lstm_scaled_single)\n",
    "\n",
    "    metrics = evaluate_model(y_true_inv_single, y_pred_inv_single, \"LSTM\", symbol)\n",
    "    lstm_results_per_stock[symbol] = {'metrics': metrics, 'y_true': y_true_inv_single, 'y_pred': y_pred_inv_single}\n",
    "\n",
    "    plot_predictions(y_true_inv_single, y_pred_inv_single, f'LSTM: {symbol} - Giá Thực tế vs. Dự đoán')\n",
    "\n",
    "    all_y_true_lstm.extend(y_true_inv_single)\n",
    "    all_y_pred_lstm.extend(y_pred_inv_single)\n",
    "\n",
    "print(\"\\n--- Đánh giá LSTM Tổng thể (Overall) ---\")\n",
    "overall_metrics_lstm = evaluate_model(np.array(all_y_true_lstm), np.array(all_y_pred_lstm), \"LSTM\", \"Overall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b859bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn1d_model(input_shape, filters=64, kernel_size=3, pool_size=2, num_cnn_blocks=2,\n",
    "                     dropout_rate=0.2, dense_units=64, l2_reg=0.001):\n",
    "    \"\"\"Xây dựng mô hình CNN1D.\"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = inputs\n",
    "\n",
    "    for i in range(num_cnn_blocks):\n",
    "        x = Conv1D(filters=filters * (2**i), kernel_size=kernel_size, activation='relu',\n",
    "                   padding='causal', kernel_regularizer=l2(l2_reg))(x)\n",
    "        if pool_size > 1:\n",
    "             x = MaxPooling1D(pool_size=pool_size, padding='same')(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = Conv1D(filters=filters * (2**num_cnn_blocks), kernel_size=kernel_size, activation='relu',\n",
    "               padding='causal', kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    x = Dense(dense_units, activation=\"relu\", kernel_regularizer=l2(l2_reg))(x)\n",
    "    x = Dropout(dropout_rate)(x)\n",
    "    outputs = Dense(1, activation=\"linear\", dtype='float32')(x)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    return model\n",
    "\n",
    "if dl_input_shape:\n",
    "    print(\"Đang xây dựng mô hình CNN1D...\")\n",
    "    cnn1d_model = build_cnn1d_model(\n",
    "        input_shape=dl_input_shape,\n",
    "        filters=32,\n",
    "        kernel_size=3,\n",
    "        pool_size=2,\n",
    "        num_cnn_blocks=3,\n",
    "        dropout_rate=0.2,\n",
    "        dense_units=64\n",
    "    )\n",
    "    print(\"\\nTổng quan Kiến trúc Mô hình CNN1D:\")\n",
    "    cnn1d_model.summary(line_length=150)\n",
    "else:\n",
    "    print(\"Bỏ qua xây dựng mô hình CNN1D: Chưa xác định được input shape.\")\n",
    "\n",
    "history_cnn1d = None\n",
    "cnn1d_model_path = os.path.join(MODEL_DIR, 'best_cnn1d_model.keras')\n",
    "\n",
    "if cnn1d_model is not None and 'train_generator_dl' in locals() and 'val_generator_dl' in locals() \\\n",
    "   and len(train_generator_dl) > 0 and len(val_generator_dl) > 0:\n",
    "\n",
    "    print(\"\\nSử dụng lại Data Generators (Sequence) cho mô hình CNN1D...\")\n",
    "    print(f\"CNN1D Train generator có {len(train_generator_dl)} batches.\")\n",
    "    print(f\"CNN1D Validation generator có {len(val_generator_dl)} batches.\")\n",
    "\n",
    "    model_checkpoint_cnn1d = ModelCheckpoint(filepath=cnn1d_model_path, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n",
    "    early_stopping_cnn1d = EarlyStopping(monitor='val_loss', patience=15, mode='min', restore_best_weights=True, verbose=1)\n",
    "    reduce_lr_cnn1d = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=7, mode='min', min_lr=1e-6, verbose=1)\n",
    "\n",
    "    epochs = 20\n",
    "    history_cnn1d = cnn1d_model.fit(\n",
    "        train_generator_dl, epochs=epochs,\n",
    "        validation_data=val_generator_dl,\n",
    "        callbacks=[model_checkpoint_cnn1d, early_stopping_cnn1d, reduce_lr_cnn1d],\n",
    "        verbose=1,\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history_cnn1d.history['loss'], label='Train Loss')\n",
    "    plt.plot(history_cnn1d.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('CNN1D: Training & Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show(block=False)\n",
    "    plt.pause(1)\n",
    "    plt.close()\n",
    "\n",
    "print(\"\\n--- Đánh giá CNN1D trên từng công ty ---\")\n",
    "cnn1d_results_per_stock = {}\n",
    "\n",
    "if os.path.exists(cnn1d_model_path) and scaled_encoded_test_data:\n",
    "    print(f\"\\nĐang tải mô hình CNN1D tốt nhất từ: {cnn1d_model_path}...\")\n",
    "    cnn1d_model_loaded = load_model(cnn1d_model_path)\n",
    "    print(\"Đã tải mô hình CNN1D.\")\n",
    "    print(\"Bắt đầu đánh giá CNN1D...\")\n",
    "\n",
    "    all_y_true_cnn1d = []\n",
    "    all_y_pred_cnn1d = []\n",
    "    cnn1d_test_batch_size = 128\n",
    "\n",
    "    if 'available_test_symbols' not in locals():\n",
    "         available_test_symbols = [s for s in STOCK_LIST if s in scaled_encoded_test_data and not scaled_encoded_test_data[s].empty]\n",
    "\n",
    "    for symbol in available_test_symbols:\n",
    "        print(f\"\\nĐang đánh giá cho mã: {symbol}\")\n",
    "        temp_test_dict = {symbol: scaled_encoded_test_data[symbol]}\n",
    "\n",
    "        cnn1d_test_generator_single = StockDataSequence(\n",
    "            data_dict=temp_test_dict, window_size=WINDOW_SIZE, horizon=HORIZON,\n",
    "            batch_size=cnn1d_test_batch_size, target_col_name=TARGET_COL_NAME,\n",
    "            feature_list=sequence_feature_names, shuffle=False\n",
    "        )\n",
    "\n",
    "        if len(cnn1d_test_generator_single) == 0:\n",
    "            print(f\"[{symbol}] Không tạo được batch nào từ generator test CNN1D. Bỏ qua.\")\n",
    "            continue\n",
    "\n",
    "        y_test_true_scaled_single = cnn1d_test_generator_single.get_all_targets()\n",
    "        if y_test_true_scaled_single.size == 0:\n",
    "             print(f\"[{symbol}] Không lấy được giá trị target nào từ generator CNN1D. Bỏ qua.\")\n",
    "             continue\n",
    "\n",
    "        y_pred_cnn1d_scaled_single = cnn1d_model_loaded.predict(cnn1d_test_generator_single)\n",
    "        if y_pred_cnn1d_scaled_single.size == 0:\n",
    "             print(f\"[{symbol}] Không tạo được dự đoán CNN1D nào. Bỏ qua.\")\n",
    "             continue\n",
    "\n",
    "        y_true_inv_single = data_processor.inverse_transform_target(y_test_true_scaled_single)\n",
    "        y_pred_inv_single = data_processor.inverse_transform_target(y_pred_cnn1d_scaled_single)\n",
    "\n",
    "        metrics = evaluate_model(y_true_inv_single, y_pred_inv_single, \"CNN1D\", symbol)\n",
    "        cnn1d_results_per_stock[symbol] = {'metrics': metrics, 'y_true': y_true_inv_single, 'y_pred': y_pred_inv_single}\n",
    "\n",
    "        plot_predictions(y_true_inv_single, y_pred_inv_single, f'CNN1D: {symbol} - Giá Thực tế vs. Dự đoán')\n",
    "\n",
    "        all_y_true_cnn1d.extend(y_true_inv_single)\n",
    "        all_y_pred_cnn1d.extend(y_pred_inv_single)\n",
    "\n",
    "    print(\"\\n--- Đánh giá CNN1D Tổng thể (Overall) ---\")\n",
    "    if all_y_true_cnn1d and all_y_pred_cnn1d:\n",
    "        overall_metrics_cnn1d = evaluate_model(np.array(all_y_true_cnn1d), np.array(all_y_pred_cnn1d), \"CNN1D\", \"Overall\")\n",
    "    else:\n",
    "        print(\"Không có đủ dữ liệu để đánh giá tổng thể cho CNN1D.\")\n",
    "\n",
    "elif not os.path.exists(cnn1d_model_path):\n",
    "    print(\"\\nBỏ qua đánh giá CNN1D: Không tìm thấy file mô hình.\")\n",
    "else:\n",
    "    print(\"\\nBỏ qua đánh giá CNN1D: Dữ liệu test chưa được xử lý hoặc trống.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4edf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "model_results_dicts = {\n",
    "    \"SGD\": sgd_results_per_stock,\n",
    "    \"CNN-BiLSTM-Att\": dl_results_per_stock,\n",
    "    \"LSTM\": lstm_results_per_stock,\n",
    "    \"CNN1D\": cnn1d_results_per_stock\n",
    "}\n",
    "\n",
    "print(\"Đang tổng hợp kết quả...\")\n",
    "for model_name, results_dict in model_results_dicts.items():\n",
    "    if results_dict:\n",
    "        for symbol, data in results_dict.items():\n",
    "            metrics = data.get('metrics', {})\n",
    "            all_results.append({\n",
    "                'Model': model_name,\n",
    "                'Symbol': symbol,\n",
    "                'R2': metrics.get('r2'),\n",
    "                'MSE': metrics.get('mse'),\n",
    "                'MAE': metrics.get('mae')\n",
    "            })\n",
    "\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    print(\"\\nBảng tổng hợp kết quả đánh giá theo từng mã cổ phiếu:\")\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):\n",
    "        print(results_df.round(4))\n",
    "\n",
    "    print(\"\\nKết quả trung bình trên tất cả các mã cổ phiếu:\")\n",
    "    numeric_cols_agg = ['R2', 'MSE', 'MAE']\n",
    "    for col in numeric_cols_agg:\n",
    "        results_df[col] = pd.to_numeric(results_df[col], errors='coerce')\n",
    "\n",
    "    avg_results = results_df.groupby('Model')[numeric_cols_agg].mean()\n",
    "    print(avg_results.round(4))\n",
    "\n",
    "    if not avg_results.empty:\n",
    "        print(\"\\nĐang vẽ biểu đồ so sánh MAE trung bình...\")\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        avg_results['MAE'].sort_values().plot(kind='bar', color=sns.color_palette(\"viridis\", len(avg_results)))\n",
    "        plt.title('So sánh MAE Trung bình của các Mô hình')\n",
    "        plt.ylabel('Mean Absolute Error (MAE)')\n",
    "        plt.xlabel('Mô hình')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.grid(axis='y', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"\\nKhông có kết quả nào được ghi nhận để tổng hợp.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
